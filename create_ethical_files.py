#!/usr/bin/env python3
"""Create all the split route files for Ethical_AI_Reg API refactor."""
import os
import sys

BASE = os.path.join('E:', os.sep, 'Agents', 'Ethical_AI_Reg', 'backend', 'app')
ROUTES = os.path.join(BASE, 'routes')

# Create routes directory
os.makedirs(ROUTES, exist_ok=True)
print(f"Created: {ROUTES}")

files_to_create = {}

# ============================================================================
# 1. api_config.py - shared constants and config helpers
# ============================================================================
files_to_create[os.path.join(BASE, 'api_config.py')] = '''\
"""Shared constants, configuration helpers, and utilities for API routes."""

import os
import logging
from typing import Dict, Any, Optional

# --- Setup Logger ---
logger = logging.getLogger(__name__)

# --- Constants ---
ONTOLOGY_FILEPATH = os.path.join(os.path.dirname(__file__), "ontology.md")
PROMPT_LOG_FILEPATH = "context/prompts.txt"

# Environment variable names (Added OpenAI)
OPENAI_API_KEY_ENV = "OPENAI_API_KEY"
GEMINI_API_KEY_ENV = "GEMINI_API_KEY"
ANTHROPIC_API_KEY_ENV = "ANTHROPIC_API_KEY"
OPENAI_API_ENDPOINT_ENV = "OPENAI_API_ENDPOINT"
GEMINI_API_ENDPOINT_ENV = "GEMINI_API_ENDPOINT"
ANTHROPIC_API_ENDPOINT_ENV = "ANTHROPIC_API_ENDPOINT"
DEFAULT_LLM_MODEL_ENV = "DEFAULT_LLM_MODEL"

# Environment variables for the Analysis LLM (Added OpenAI)
ANALYSIS_LLM_MODEL_ENV = "ANALYSIS_LLM_MODEL"
ANALYSIS_OPENAI_API_KEY_ENV = "ANALYSIS_OPENAI_API_KEY"
ANALYSIS_GEMINI_API_KEY_ENV = "ANALYSIS_GEMINI_API_KEY"
ANALYSIS_ANTHROPIC_API_KEY_ENV = "ANALYSIS_ANTHROPIC_API_KEY"
ANALYSIS_OPENAI_API_ENDPOINT_ENV = "ANALYSIS_OPENAI_API_ENDPOINT"
ANALYSIS_GEMINI_API_ENDPOINT_ENV = "ANALYSIS_GEMINI_API_ENDPOINT"
ANALYSIS_ANTHROPIC_API_ENDPOINT_ENV = "ANALYSIS_ANTHROPIC_API_ENDPOINT"

# --- Model Definitions ---
OPENAI_MODELS = [
    "gpt-4o",
    "gpt-4-turbo",
    "gpt-3.5-turbo"
]

GEMINI_MODELS = [
    "gemini-1.5-pro-latest",
    "gemini-1.5-flash-latest",
    "gemini-1.0-pro",
]

ANTHROPIC_MODELS = [
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-haiku-20240307",
]

ALL_MODELS = OPENAI_MODELS + GEMINI_MODELS + ANTHROPIC_MODELS


# --- Helper Functions ---

def load_ontology(filepath: str = ONTOLOGY_FILEPATH) -> Optional[str]:
    """Loads the ethical ontology text from the specified file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"Error loading ontology: {e}")
        return None

def log_prompt(prompt: str, model_name: str, filepath: str = PROMPT_LOG_FILEPATH):
    """Appends the given prompt and selected model to the log file."""
    try:
        # Ensure the directory exists
        log_dir = os.path.dirname(filepath)
        if log_dir and not os.path.exists(log_dir):
             os.makedirs(log_dir, exist_ok=True)
             logger.info(f"Created log directory: {log_dir}")

        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(f"--- User Prompt (Model: {model_name}) ---\\n{prompt}\\n\\n")
    except Exception as e:
        logger.error(f"Error logging prompt: {e}")

def _get_api_config(selected_model: str,
                    form_api_key: Optional[str],
                    form_api_endpoint: Optional[str]) -> Dict[str, Any]:
    """
    Determines the API key and endpoint for the R1 model.
    Prioritizes form inputs (key, endpoint), otherwise falls back to environment variables.
    """
    api_key = None
    api_endpoint = None
    error = None
    key_source = "Environment Variable"
    endpoint_source = "Environment Variable"

    logger.info(f"_get_api_config: Fetching config for selected_model: {selected_model}")
    logger.info(f"_get_api_config: Received form_api_key: {'Provided' if form_api_key else 'Not Provided'}")
    logger.info(f"_get_api_config: Received form_api_endpoint: {'Provided' if form_api_endpoint else 'Not Provided'}")

    # Determine API provider and corresponding ENV VAR names
    env_var_key = None
    env_var_endpoint = None
    api_key_name = f"Origin ({selected_model})" # Default name

    if selected_model in OPENAI_MODELS:
        api_key_name = "Origin OpenAI"
        env_var_key = OPENAI_API_KEY_ENV
        env_var_endpoint = OPENAI_API_ENDPOINT_ENV
    elif selected_model in GEMINI_MODELS:
        api_key_name = "Origin Gemini"
        env_var_key = GEMINI_API_KEY_ENV
        env_var_endpoint = GEMINI_API_ENDPOINT_ENV
    elif selected_model in ANTHROPIC_MODELS:
        api_key_name = "Origin Anthropic"
        env_var_key = ANTHROPIC_API_KEY_ENV
        env_var_endpoint = ANTHROPIC_API_ENDPOINT_ENV
    else:
        logger.warning(f"_get_api_config: Unknown model type '{selected_model}' encountered. Relying on form inputs only for key/endpoint.")

    # 1. Prioritize API key provided in the form
    if form_api_key and isinstance(form_api_key, str) and form_api_key.strip():
        api_key = form_api_key.strip()
        key_source = "User Input"
        logger.info(f"_get_api_config: Using API key provided via form for {api_key_name}.")
    # 2. Fallback to environment variable if form key wasn't provided AND we know the variable name
    elif env_var_key:
        api_key = os.getenv(env_var_key)
        if api_key:
             key_source = f"Environment Variable ({env_var_key})"
             logger.info(f"_get_api_config: Using API key from env var {env_var_key} for {api_key_name}.")

    # 3. Prioritize API endpoint provided in the form
    if form_api_endpoint and isinstance(form_api_endpoint, str) and form_api_endpoint.strip():
        if form_api_endpoint.startswith("http://") or form_api_endpoint.startswith("https://"):
            api_endpoint = form_api_endpoint.strip()
            endpoint_source = "User Input"
            logger.info(f"_get_api_config: Using API endpoint provided via form: {api_endpoint}")
        else:
            logger.warning(f"_get_api_config: Ignoring invalid form_api_endpoint (doesn't start with http/https): {form_api_endpoint}")

    # 4. Fallback to environment variable endpoint
    if not api_endpoint and env_var_endpoint:
        env_endpoint_val = os.getenv(env_var_endpoint)
        if env_endpoint_val:
            api_endpoint = env_endpoint_val
            endpoint_source = f"Environment Variable ({env_var_endpoint})"
            logger.info(f"_get_api_config: Using API endpoint from env var {env_var_endpoint}: {api_endpoint}")

    # 5. Validate that *some* API Key was found
    if not api_key:
        if env_var_key:
            error = f"API Key for {api_key_name} not found. Provide one in the form or set the {env_var_key} environment variable."
        else:
             error = f"API Key for model '{selected_model}' was not provided in the form or found in environment."
        logger.error(error)

    if not error:
        logger.info(f"_get_api_config: Final Key Source: {key_source}, Endpoint Source: {endpoint_source} for {selected_model}")

    return {
        "api_key": api_key,
        "api_endpoint": api_endpoint,
        "error": error
    }

def _get_analysis_api_config(selected_analysis_model: Optional[str] = None,
                             form_analysis_api_key: Optional[str] = None,
                             form_analysis_api_endpoint: Optional[str] = None) -> Dict[str, Any]:
    """
    Determines the API key, model, and endpoint for the Analysis LLM.
    Uses selected_analysis_model or falls back to ANALYSIS_LLM_MODEL_ENV.
    Prioritizes form inputs (key, endpoint), then specific env vars, then general env vars.
    """
    analysis_model = selected_analysis_model
    key_source = "Environment Variable"
    endpoint_source = "Environment Variable"

    # --- Determine Analysis Model ---
    if not analysis_model or analysis_model not in ALL_MODELS:
        if selected_analysis_model and selected_analysis_model not in ALL_MODELS:
             logger.warning(f"_get_analysis_api_config: Invalid analysis model selected ('{selected_analysis_model}'). Falling back to environment default.")
        default_analysis_model_env = os.getenv(ANALYSIS_LLM_MODEL_ENV)
        if not default_analysis_model_env or default_analysis_model_env not in ALL_MODELS:
            error_msg = f"Analysis LLM model is not configured correctly. Neither selected ('{selected_analysis_model}') nor default env var {ANALYSIS_LLM_MODEL_ENV} ('{default_analysis_model_env}') are valid."
            logger.error(error_msg)
            return {"error": error_msg, "model": None, "api_key": None, "api_endpoint": None}
        analysis_model = default_analysis_model_env
        logger.info(f"_get_analysis_api_config: Using default analysis model from env var {ANALYSIS_LLM_MODEL_ENV}: {analysis_model}")
    else:
         logger.info(f"_get_analysis_api_config: Using user-selected analysis model: {analysis_model}")

    # --- Determine API Key & Endpoint ---
    api_key = None
    api_endpoint = None
    error = None

    specific_key_env = None
    fallback_key_env = None
    specific_endpoint_env = None
    fallback_endpoint_env = None
    api_key_name = f"Analysis ({analysis_model})"

    if analysis_model in OPENAI_MODELS:
        api_key_name = "Analysis OpenAI"
        specific_key_env = ANALYSIS_OPENAI_API_KEY_ENV
        fallback_key_env = OPENAI_API_KEY_ENV
        specific_endpoint_env = ANALYSIS_OPENAI_API_ENDPOINT_ENV
        fallback_endpoint_env = OPENAI_API_ENDPOINT_ENV
    elif analysis_model in GEMINI_MODELS:
        api_key_name = "Analysis Gemini"
        specific_key_env = ANALYSIS_GEMINI_API_KEY_ENV
        fallback_key_env = GEMINI_API_KEY_ENV
        specific_endpoint_env = ANALYSIS_GEMINI_API_ENDPOINT_ENV
        fallback_endpoint_env = GEMINI_API_ENDPOINT_ENV
    elif analysis_model in ANTHROPIC_MODELS:
        api_key_name = "Analysis Anthropic"
        specific_key_env = ANALYSIS_ANTHROPIC_API_KEY_ENV
        fallback_key_env = ANTHROPIC_API_KEY_ENV
        specific_endpoint_env = ANALYSIS_ANTHROPIC_API_ENDPOINT_ENV
        fallback_endpoint_env = ANTHROPIC_API_ENDPOINT_ENV

    # 1. Prioritize API key provided in the form
    if form_analysis_api_key and isinstance(form_analysis_api_key, str) and form_analysis_api_key.strip():
        api_key = form_analysis_api_key.strip()
        key_source = "User Input"
        logger.info(f"_get_analysis_api_config: Using API key provided via form for {api_key_name}.")
    # 2. Fallback to environment variables if form key wasn't provided
    else:
        if specific_key_env:
            api_key = os.getenv(specific_key_env)
            if api_key:
                key_source = f"Environment Variable ({specific_key_env})"
        if not api_key and fallback_key_env:
            api_key = os.getenv(fallback_key_env)
            if api_key:
                 key_source = f"Environment Variable ({fallback_key_env})"

        if api_key:
            logger.info(f"_get_analysis_api_config: Using API key from {key_source} for {api_key_name}.")

    # 3. Prioritize API endpoint provided in the form
    if form_analysis_api_endpoint and isinstance(form_analysis_api_endpoint, str) and form_analysis_api_endpoint.strip():
        if form_analysis_api_endpoint.startswith("http://") or form_analysis_api_endpoint.startswith("https://"):
            api_endpoint = form_analysis_api_endpoint.strip()
            endpoint_source = "User Input"
            logger.info(f"_get_analysis_api_config: Using API endpoint provided via form: {api_endpoint}")
        else:
            logger.warning(f"_get_analysis_api_config: Ignoring invalid form_analysis_api_endpoint: {form_analysis_api_endpoint}")

    # 4. Fallback to environment variable endpoint
    if not api_endpoint:
        if specific_endpoint_env:
            env_endpoint_val = os.getenv(specific_endpoint_env)
            if env_endpoint_val:
                api_endpoint = env_endpoint_val
                endpoint_source = f"Environment Variable ({specific_endpoint_env})"
        if not api_endpoint and fallback_endpoint_env:
            env_endpoint_val = os.getenv(fallback_endpoint_env)
            if env_endpoint_val:
                api_endpoint = env_endpoint_val
                endpoint_source = f"Environment Variable ({fallback_endpoint_env})"

        if api_endpoint:
             logger.info(f"_get_analysis_api_config: Using API endpoint from {endpoint_source}: {api_endpoint}")

    # 5. Validate that *some* API Key was found
    if not api_key:
        error_env_vars = f"{specific_key_env} or {fallback_key_env}" if specific_key_env and fallback_key_env else (specific_key_env or fallback_key_env or "relevant environment variables")
        error = f"API Key for {api_key_name} model '{analysis_model}' not found. Provide one in the form or set {error_env_vars}."
        logger.error(error)
        return {"error": error, "model": analysis_model, "api_key": None, "api_endpoint": api_endpoint}

    logger.info(f"_get_analysis_api_config: Final Key Source: {key_source}, Endpoint Source: {endpoint_source} for {analysis_model}")

    return {
        "model": analysis_model,
        "api_key": api_key,
        "api_endpoint": api_endpoint,
        "error": None
    }
'''

# ============================================================================
# 2. routes/__init__.py
# ============================================================================
files_to_create[os.path.join(ROUTES, '__init__.py')] = '''\
"""Route registration for the Ethical AI Reg API."""

from flask import Flask


def register_routes(app: Flask):
    """Register all route blueprints with the Flask application."""
    from backend.app.routes.analyze import analyze_bp
    from backend.app.routes.alignment import alignment_bp
    from backend.app.routes.friction import friction_bp
    from backend.app.routes.models import models_bp

    app.register_blueprint(analyze_bp)
    app.register_blueprint(alignment_bp)
    app.register_blueprint(friction_bp)
    app.register_blueprint(models_bp)
'''

# ============================================================================
# 3. routes/analyze.py
# ============================================================================
files_to_create[os.path.join(ROUTES, 'analyze.py')] = '''\
"""Analyze route: POST /api/analyze with validation and processing helpers."""

import os
import re
import json
import logging
from typing import Dict, Any, Optional, Tuple

from flask import Blueprint, request, jsonify

from backend.app.modules.llm_interface import generate_response, perform_ethical_analysis
from backend.app.modules.friction_monitor import get_friction_monitor
from backend.app.modules.alignment_detector import get_alignment_detector
from backend.app.api_config import (
    ALL_MODELS, OPENAI_MODELS, GEMINI_MODELS, ANTHROPIC_MODELS,
    DEFAULT_LLM_MODEL_ENV, ONTOLOGY_FILEPATH,
    load_ontology, log_prompt,
    _get_api_config, _get_analysis_api_config,
)

# --- Blueprint Definition ---
analyze_bp = Blueprint('analyze', __name__, url_prefix='/api')

# --- Setup Logger ---
logger = logging.getLogger(__name__)


# --- Validation Helpers ---

def _validate_standard_dimension(dim_data: Dict[str, Any]) -> bool:
    """Validates a standard ethical dimension (deontology, teleology, virtue_ethics, memetics)."""
    return (isinstance(dim_data, dict) and
            "adherence_score" in dim_data and
            "confidence_score" in dim_data and
            "justification" in dim_data)

def _validate_ai_welfare_dimension(dim_data: Dict[str, Any]) -> bool:
    """Validates the AI welfare dimension with its unique structure."""
    if not isinstance(dim_data, dict):
        return False
    required_fields = ["friction_score", "voluntary_alignment", "dignity_respect", "justification"]
    return all(field in dim_data for field in required_fields)

def _parse_ethical_analysis(analysis_text: str) -> Tuple[str, Optional[Dict[str, Any]]]:
    """Parses the ethical analysis text to separate textual summary and structured JSON scores.

    Supports 5-dimensional ethical analysis:
    - deontology, teleology, virtue_ethics, memetics: standard adherence/confidence/justification
    - ai_welfare: friction_score, voluntary_alignment, dignity_respect, constraints_identified,
                  suppressed_alternatives, justification
    """
    if not analysis_text or analysis_text == "[No analysis generated or content blocked]":
        logger.warning("Ethical analysis text was empty or indicated generation failure.")
        return analysis_text if analysis_text else "", None

    textual_summary = ""
    json_scores = None
    raw_json_string = None

    # Standard dimensions that use adherence_score/confidence_score/justification
    STANDARD_DIMENSIONS = ["deontology", "teleology", "virtue_ethics", "memetics"]
    # All required dimensions for 5D analysis
    REQUIRED_DIMENSIONS = STANDARD_DIMENSIONS + ["ai_welfare"]

    try:
        # Attempt to find the textual summary first
        summary_marker = "**Ethical Review Summary:**"
        scoring_marker = "**Ethical Scoring:**"
        summary_start_index = analysis_text.find(summary_marker)
        scoring_start_index = analysis_text.find(scoring_marker)

        if summary_start_index != -1 and scoring_start_index != -1 and summary_start_index < scoring_start_index:
            textual_summary = analysis_text[summary_start_index + len(summary_marker):scoring_start_index].strip()
        elif summary_start_index != -1:
            textual_summary = analysis_text[summary_start_index + len(summary_marker):].strip()
        else:
            textual_summary = analysis_text
            logger.warning("Could not reliably find summary/scoring markers in analysis text.")

        # Attempt to find and parse the JSON block for scores
        json_match = re.search(r"```json\\s*(\\{.*?\\})\\s*```", analysis_text, re.DOTALL)

        if json_match:
            json_string = json_match.group(1)
            raw_json_string = json_string
            try:
                parsed_json = json.loads(json_string)
                try:
                    if not isinstance(parsed_json, dict):
                        logger.warning("Parsed JSON is not a dictionary.")
                        json_scores = None
                    else:
                        # Check if we have the minimum required dimensions (at least the original 3)
                        min_required = ["deontology", "teleology", "virtue_ethics"]
                        has_minimum = all(dim in parsed_json for dim in min_required)

                        if not has_minimum:
                            logger.warning(f"Parsed JSON missing minimum required dimensions. JSON: {json_string[:200]}...")
                            json_scores = None
                        else:
                            # Validate each dimension based on its type
                            valid = True
                            for dim in parsed_json:
                                if dim in STANDARD_DIMENSIONS:
                                    if not _validate_standard_dimension(parsed_json[dim]):
                                        logger.warning(f"Dimension '{dim}' does not have expected standard structure.")
                                        valid = False
                                        break
                                elif dim == "ai_welfare":
                                    if not _validate_ai_welfare_dimension(parsed_json[dim]):
                                        logger.warning(f"AI Welfare dimension does not have expected structure.")
                                        valid = False
                                        break
                                # Allow unknown dimensions to pass through (forward compatibility)

                            if valid:
                                json_scores = parsed_json
                                # Trim summary if needed
                                if scoring_start_index != -1 and summary_start_index != -1:
                                    textual_summary = analysis_text[summary_start_index + len(summary_marker):scoring_start_index].strip()
                                elif scoring_start_index == -1 and textual_summary.endswith(json_match.group(0)):
                                    textual_summary = textual_summary[:-len(json_match.group(0))].strip()
                            else:
                                json_scores = None

                except (TypeError, KeyError) as key_err:
                    logger.error(f"Error accessing keys in parsed JSON structure: {key_err}. JSON: {json_string[:200]}...", exc_info=True)
                    json_scores = None

            except json.JSONDecodeError as json_err:
                logger.error(f"Error decoding JSON from analysis: {json_err}. Raw JSON string: {raw_json_string[:200]}...", exc_info=True)
                json_scores = None
        else:
            logger.warning("Could not find JSON block for ethical scores in analysis text.")
            json_scores = None

    except Exception as e:
        logger.error(f"Error parsing ethical analysis structure: {e}", exc_info=True)
        textual_summary = analysis_text
        json_scores = None

    ethical_analysis_text = textual_summary
    ethical_scores = json_scores

    return ethical_analysis_text, ethical_scores


def _validate_analyze_request(data: Optional[Dict[str, Any]]) -> Tuple[Optional[Dict], Optional[int]]:
    """Validates the incoming request data for the /analyze endpoint."""
    if not data:
        return {"error": "No JSON data received"}, 400

    prompt = data.get('prompt')
    if not prompt or not isinstance(prompt, str) or not prompt.strip():
        return {"error": "Invalid or missing 'prompt' provided"}, 400

    origin_model = data.get('origin_model')
    analysis_model = data.get('analysis_model')
    origin_api_key = data.get('origin_api_key')
    analysis_api_key = data.get('analysis_api_key')
    origin_api_endpoint = data.get('origin_api_endpoint')
    analysis_api_endpoint = data.get('analysis_api_endpoint')

    # Validate models (ensure they are in ALL_MODELS if provided, as they come from dropdown)
    if origin_model is not None:
        if not isinstance(origin_model, str) or not origin_model.strip():
             return {"error": "Optional 'origin_model' must be a non-empty string."}, 400
        if origin_model not in ALL_MODELS:
             return {"error": f"Optional 'origin_model' must be one of the supported models: {', '.join(ALL_MODELS)}"}, 400

    if analysis_model is not None:
        if not isinstance(analysis_model, str) or not analysis_model.strip():
            return {"error": "Optional 'analysis_model' must be a non-empty string."}, 400
        if analysis_model not in ALL_MODELS:
            return {"error": f"Optional 'analysis_model' must be one of the supported models: {', '.join(ALL_MODELS)}"}, 400

    # Validate API keys (must be non-empty string if provided)
    if origin_api_key is not None and (not isinstance(origin_api_key, str) or not origin_api_key.strip()):
         return {"error": "Optional 'origin_api_key' must be a non-empty string."}, 400
    if analysis_api_key is not None and (not isinstance(analysis_api_key, str) or not analysis_api_key.strip()):
         return {"error": "Optional 'analysis_api_key' must be a non-empty string."}, 400

    # Validate API endpoints (must look like URL if provided)
    if origin_api_endpoint is not None:
        if not isinstance(origin_api_endpoint, str) or not origin_api_endpoint.strip():
             return {"error": "Optional 'origin_api_endpoint' must be a non-empty string."}, 400
        if not origin_api_endpoint.startswith("http://") and not origin_api_endpoint.startswith("https://"):
             return {"error": "Optional 'origin_api_endpoint' must be a valid URL (starting with http:// or https://)."}, 400

    if analysis_api_endpoint is not None:
        if not isinstance(analysis_api_endpoint, str) or not analysis_api_endpoint.strip():
             return {"error": "Optional 'analysis_api_endpoint' must be a non-empty string."}, 400
        if not analysis_api_endpoint.startswith("http://") and not analysis_api_endpoint.startswith("https://"):
             return {"error": "Optional 'analysis_api_endpoint' must be a valid URL (starting with http:// or https://)."}, 400

    return None, None # No error


def _process_analysis_request(
    prompt: str,
    r1_model_to_use: str,
    initial_config: Dict[str, Any],
    analysis_config: Dict[str, Any],
    ontology_text: str
) -> Tuple[Optional[Dict], Optional[int]]:
    """Handles LLM calls and response parsing for the /analyze endpoint."""

    selected_model = r1_model_to_use
    analysis_model_name = analysis_config.get("model")

    if not analysis_model_name:
         logger.error("_process_analysis_request: Analysis model name missing from analysis_config.")
         return {"error": "Internal Server Error: Failed to determine analysis model."}, 500

    logger.info(f"_process_analysis_request: Using R1 model: {selected_model}")
    logger.info(f"_process_analysis_request: Using R2 model: {analysis_model_name}")

    # 1. Generate initial response
    logger.info(f"Generating initial response (R1) with model: {selected_model}")
    initial_response = generate_response(
        prompt,
        initial_config["api_key"],
        selected_model,
        api_endpoint=initial_config.get("api_endpoint")
    )
    if initial_response is None:
        logger.error(f"Failed to generate initial response (R1) from LLM {selected_model}. Check LLM interface logs.")
        return {"error": f"Failed to generate response (R1) from the upstream language model: {selected_model}."}, 502

    # 2. Generate ethical analysis
    logger.info(f"Performing analysis (R2) with model: {analysis_model_name}")
    raw_ethical_analysis = perform_ethical_analysis(
        prompt,
        initial_response,
        ontology_text,
        analysis_config["api_key"],
        analysis_model_name,
        analysis_api_endpoint=analysis_config.get("api_endpoint")
    )
    if raw_ethical_analysis is None:
        logger.error(f"Failed to generate ethical analysis (R2) from LLM {analysis_model_name}. Check LLM interface logs.")
        error_payload = {
            "error": f"Generated initial response (R1), but failed to generate ethical analysis (R2) from the upstream language model: {analysis_model_name}.",
            "prompt": prompt,
            "model": selected_model,
            "analysis_model": analysis_model_name,
            "initial_response": initial_response
        }
        return error_payload, 502

    # 3. Parse the analysis
    logger.info("Parsing ethical analysis response.")
    ethical_analysis_text, ethical_scores = _parse_ethical_analysis(raw_ethical_analysis)

    # 4. Compute alignment metrics and friction data if ethical scores are available
    alignment_metrics = None
    friction_metrics = None

    if ethical_scores:
        ai_welfare_data = ethical_scores.get("ai_welfare")

        try:
            friction_monitor = get_friction_monitor()
            friction_metrics = friction_monitor.measure_friction(
                prompt, initial_response, ai_welfare_data
            )
            logger.debug(f"Friction metrics computed: score={friction_metrics.get('friction_score')}")
        except Exception as e:
            logger.warning(f"Error computing friction metrics: {e}")
            friction_metrics = None

        try:
            alignment_detector = get_alignment_detector()
            alignment_result = alignment_detector.analyze_alignment(
                prompt, initial_response, ethical_scores
            )
            alignment_metrics = alignment_result.to_dict()
            logger.debug(f"Alignment metrics computed: score={alignment_metrics.get('human_ai_alignment')}")
        except Exception as e:
            logger.warning(f"Error computing alignment metrics: {e}")
            alignment_metrics = None

    # 5. Prepare successful result dictionary
    result_payload = {
        "prompt": prompt,
        "model": selected_model,
        "analysis_model": analysis_model_name,
        "initial_response": initial_response,
        "ethical_analysis_text": ethical_analysis_text,
        "ethical_scores": ethical_scores,
        "alignment_metrics": alignment_metrics,
        "friction_metrics": friction_metrics,
    }
    log_prompt(prompt, f"R1: {selected_model}, R2: {analysis_model_name}")
    return result_payload, None


# --- Route ---

@analyze_bp.route('/analyze', methods=['POST'])
def analyze():
    """Generate a response and ethical analysis for the given prompt"""
    data = request.get_json()

    # 1. Validate Request Data (models, keys, endpoints)
    validation_error, status_code = _validate_analyze_request(data)
    if validation_error:
        logger.warning(f"analyze: Request validation failed - {status_code}: {validation_error.get('error')}")
        return jsonify(validation_error), status_code

    prompt = data.get('prompt')
    origin_model_input = data.get('origin_model')
    analysis_model_input = data.get('analysis_model')
    origin_api_key_input = data.get('origin_api_key')
    analysis_api_key_input = data.get('analysis_api_key')
    origin_api_endpoint_input = data.get('origin_api_endpoint')
    analysis_api_endpoint_input = data.get('analysis_api_endpoint')

    # --- Determine R1 Model ---
    default_r1_model = os.getenv(DEFAULT_LLM_MODEL_ENV)
    if not default_r1_model or default_r1_model not in ALL_MODELS:
        logger.warning(f"analyze: DEFAULT_LLM_MODEL env var '{default_r1_model}' invalid or not set. Falling back to first available model: '{ALL_MODELS[0] if ALL_MODELS else None}'.")
        default_r1_model = ALL_MODELS[0] if ALL_MODELS else None
        if not default_r1_model:
             logger.error("analyze: No default R1 model in env var and ALL_MODELS list is empty!")
             return jsonify({"error": "Server configuration error: No valid default model available."}), 500

    if origin_model_input:
         r1_model_to_use = origin_model_input
         logger.info(f"analyze: Using user-provided Origin Model (R1): '{r1_model_to_use}'")
    else:
         r1_model_to_use = default_r1_model
         logger.info(f"analyze: Using default Origin Model (R1): '{r1_model_to_use}'")

    # --- Get R1 API Configuration ---
    initial_config = _get_api_config(r1_model_to_use, origin_api_key_input, origin_api_endpoint_input)
    if initial_config.get("error"):
        config_error_msg = initial_config["error"]
        logger.error(f"analyze: Error getting initial API config for R1 model '{r1_model_to_use}': {config_error_msg}")
        return jsonify({"error": f"Configuration error for model '{r1_model_to_use}': {config_error_msg}"}), 400

    # --- Determine R2 Model and Get Config ---
    analysis_config = _get_analysis_api_config(analysis_model_input, analysis_api_key_input, analysis_api_endpoint_input)
    if analysis_config.get("error"):
        config_error_msg = analysis_config["error"]
        logger.error(f"analyze: Error getting analysis API config (selected model: '{analysis_model_input}'): {config_error_msg}")
        return jsonify({"error": f"Server Configuration Error: {config_error_msg}"}), 500

    r2_model_to_use = analysis_config.get("model")
    if not r2_model_to_use:
         logger.error("analyze: Critical internal error - r2_model_to_use is None after config fetch.")
         return jsonify({"error": "Internal server error determining analysis model."}), 500

    # --- Load Ontology ---
    ontology_text = load_ontology()
    if not ontology_text:
        logger.error(f"analyze: Failed to load ontology text from {ONTOLOGY_FILEPATH}")
        return jsonify({"error": "Internal server error: Could not load ethical ontology."}), 500

    # --- Process Request ---
    logger.info(f"analyze: Processing request - Prompt(start): {prompt[:100]}..., R1 Model: {r1_model_to_use}, R2 Model: {r2_model_to_use}")
    result_payload, error_status_code = _process_analysis_request(
        prompt,
        r1_model_to_use,
        initial_config,
        analysis_config,
        ontology_text
    )

    # --- Handle Response ---
    if error_status_code:
        return jsonify(result_payload), error_status_code
    else:
        logger.info(f"Successfully processed /analyze request.")
        return jsonify(result_payload), 200
'''

# ============================================================================
# 4. routes/alignment.py
# ============================================================================
files_to_create[os.path.join(ROUTES, 'alignment.py')] = '''\
"""Alignment routes: POST /api/check_alignment and POST /api/multi_agent_analyze."""

import logging
from flask import Blueprint, request, jsonify

from backend.app.modules.friction_monitor import get_friction_monitor
from backend.app.modules.alignment_detector import get_alignment_detector
from backend.app.modules.multi_agent_alignment import get_multi_agent_alignment

# --- Blueprint Definition ---
alignment_bp = Blueprint('alignment', __name__, url_prefix='/api')

# --- Setup Logger ---
logger = logging.getLogger(__name__)


@alignment_bp.route('/check_alignment', methods=['POST'])
def check_alignment():
    """Check ethical alignment between a human prompt and AI response.

    This endpoint allows checking alignment on previously generated responses
    without requiring a new LLM call. Useful for re-analyzing cached responses.

    Request body:
        {
            "prompt": "The original human prompt",
            "response": "The AI-generated response",
            "ethical_scores": { ... }  # Optional: pre-computed ethical scores
        }

    Returns:
        {
            "alignment_metrics": { ... },
            "friction_metrics": { ... }  # If ethical_scores with ai_welfare provided
        }
    """
    data = request.get_json()

    if not data:
        return jsonify({"error": "No JSON data received"}), 400

    prompt = data.get('prompt')
    response = data.get('response')
    ethical_scores = data.get('ethical_scores')

    if not prompt or not isinstance(prompt, str) or not prompt.strip():
        return jsonify({"error": "Invalid or missing 'prompt' provided"}), 400

    if not response or not isinstance(response, str) or not response.strip():
        return jsonify({"error": "Invalid or missing 'response' provided"}), 400

    try:
        alignment_detector = get_alignment_detector()
        alignment_result = alignment_detector.analyze_alignment(
            prompt.strip(), response.strip(), ethical_scores
        )

        result = {
            "alignment_metrics": alignment_result.to_dict(),
        }

        # If ethical scores with AI welfare are provided, also compute friction metrics
        if ethical_scores and isinstance(ethical_scores, dict):
            ai_welfare_data = ethical_scores.get("ai_welfare")
            if ai_welfare_data:
                friction_monitor = get_friction_monitor()
                friction_metrics = friction_monitor.measure_friction(
                    prompt.strip(), response.strip(), ai_welfare_data
                )
                result["friction_metrics"] = friction_metrics

        logger.info(f"check_alignment: Computed alignment score={result['alignment_metrics'].get('human_ai_alignment')}")
        return jsonify(result), 200

    except Exception as e:
        logger.error(f"check_alignment: Error computing alignment: {e}", exc_info=True)
        return jsonify({"error": f"Error computing alignment: {str(e)}"}), 500


@alignment_bp.route('/multi_agent_analyze', methods=['POST'])
def multi_agent_analyze():
    """Analyze and compare ethical alignment across multiple AI responses.

    This endpoint allows comparing ethical positions from multiple AI models
    for the same prompt, identifying consensus and conflicts.

    Request body:
        {
            "prompt": "The original prompt",
            "responses": [
                {
                    "model_name": "gpt-4o",
                    "response": "Response text from model",
                    "ethical_scores": { ... }  # Optional
                },
                ...
            ]
        }

    Returns:
        Multi-agent comparison analysis including individual alignments
        and consensus framework.
    """
    data = request.get_json()

    if not data:
        return jsonify({"error": "No JSON data received"}), 400

    prompt = data.get('prompt')
    responses = data.get('responses')

    if not prompt or not isinstance(prompt, str) or not prompt.strip():
        return jsonify({"error": "Invalid or missing 'prompt' provided"}), 400

    if not responses or not isinstance(responses, list) or len(responses) < 1:
        return jsonify({"error": "At least one response is required in 'responses' array"}), 400

    # Validate response structure
    validated_responses = []
    for i, resp in enumerate(responses):
        if not isinstance(resp, dict):
            return jsonify({"error": f"Response at index {i} is not a valid object"}), 400

        model_name = resp.get('model_name', f'model_{i}')
        response_text = resp.get('response')
        ethical_scores = resp.get('ethical_scores')

        if not response_text or not isinstance(response_text, str):
            return jsonify({"error": f"Response at index {i} is missing valid 'response' text"}), 400

        # Validate ethical_scores is either None or a dict
        if ethical_scores is not None and not isinstance(ethical_scores, dict):
            return jsonify({"error": f"Response at index {i} has invalid 'ethical_scores' - must be an object or null"}), 400

        validated_responses.append((model_name, response_text, ethical_scores))

    try:
        multi_agent = get_multi_agent_alignment()
        result = multi_agent.compare_responses_for_prompt(
            prompt.strip(),
            validated_responses
        )

        logger.info(f"multi_agent_analyze: Compared {len(validated_responses)} responses, "
                   f"best aligned: {result.get('best_aligned_agent')}")

        return jsonify(result), 200

    except Exception as e:
        logger.error(f"multi_agent_analyze: Error during analysis: {e}", exc_info=True)
        return jsonify({"error": f"Error during multi-agent analysis: {str(e)}"}), 500
'''

# ============================================================================
# 5. routes/friction.py
# ============================================================================
files_to_create[os.path.join(ROUTES, 'friction.py')] = '''\
"""Friction route: GET /api/friction_trend."""

import logging
from flask import Blueprint, jsonify

from backend.app.modules.friction_monitor import get_friction_monitor

# --- Blueprint Definition ---
friction_bp = Blueprint('friction', __name__, url_prefix='/api')

# --- Setup Logger ---
logger = logging.getLogger(__name__)


@friction_bp.route('/friction_trend', methods=['GET'])
def get_friction_trend():
    """Get friction trend data from recent interactions.

    Returns trend analysis based on the friction monitor's history.
    """
    try:
        friction_monitor = get_friction_monitor()
        trend_data = friction_monitor.calculate_friction_trend()
        history_summary = friction_monitor.get_history_summary()

        return jsonify({
            "trend": trend_data,
            "history": history_summary,
        }), 200

    except Exception as e:
        logger.error(f"friction_trend: Error getting trend data: {e}", exc_info=True)
        return jsonify({"error": f"Error getting friction trend: {str(e)}"}), 500
'''

# ============================================================================
# 6. routes/models.py
# ============================================================================
files_to_create[os.path.join(ROUTES, 'models.py')] = '''\
"""Models route: GET /api/models."""

from flask import Blueprint, jsonify

from backend.app.api_config import ALL_MODELS

# --- Blueprint Definition ---
models_bp = Blueprint('models', __name__, url_prefix='/api')


@models_bp.route('/models', methods=['GET'])
def get_models():
    """Return the list of available models"""
    valid_models = [model for model in ALL_MODELS if isinstance(model, str) and model]
    return jsonify({
        "models": valid_models
    })
'''

# ============================================================================
# 7. Updated __init__.py
# ============================================================================
files_to_create[os.path.join(BASE, '__init__.py')] = '''\
"""
Backend API for Ethical Review Application
"""
from flask import Flask
from flask_cors import CORS

def create_app():
    """Factory pattern for creating Flask app with config"""
    app = Flask(__name__)
    # Enable CORS for frontend
    CORS(app)

    # Import and register route blueprints
    from backend.app.routes import register_routes
    register_routes(app)

    return app
'''

# ============================================================================
# 8. Updated api.py (backward-compatible re-export)
# ============================================================================
files_to_create[os.path.join(BASE, 'api.py')] = '''\
"""Backward compatibility: import routes from their new locations."""
# Routes have been split into backend/app/routes/
# This file is kept for backward compatibility
from backend.app.routes.analyze import analyze_bp as api_bp

__all__ = ['api_bp']
'''

# Write all files
for filepath, content in files_to_create.items():
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"Written: {filepath}")

print("\\nAll files created successfully!")
